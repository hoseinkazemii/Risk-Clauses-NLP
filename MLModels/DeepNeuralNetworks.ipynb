{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26TdxQWlY2Zf"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8k89ncHCdBn3",
    "outputId": "4b885dba-25aa-4d6a-e564-486b82d142d7"
   },
   "outputs": [],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy_rFyz9YlHC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# import gensim.models.keyedvectors as word2vec\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# import hazm\n",
    "# from hazm import Normalizer, Stemmer, Lemmatizer, word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Flatten, Dropout, Activation\n",
    "from keras.layers import Bidirectional#, GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential#, Model\n",
    "# from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BhJDuVdfoeW"
   },
   "source": [
    "**Preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN90BaGtZaDp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Construction Management/Thesis/Data/contracts/Risky_Clauses/Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "582O4FpmgAGm",
    "outputId": "46f5d295-facb-4fdd-b4e6-366d16c93d20"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['risk'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy-J9fnhgEy4"
   },
   "source": [
    "**Split Data to X and y:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoXYq-t4gK3W"
   },
   "outputs": [],
   "source": [
    "X = df['clause']\n",
    "y = df['risk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Lf0S5Q6gS5t"
   },
   "source": [
    "**Tokenization:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApwJ2dFceU7-"
   },
   "source": [
    "**With hazm word_tokenize() method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9ZQ6-SFeeHN"
   },
   "outputs": [],
   "source": [
    "tokenizer = hazm.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMkqhv57fvRZ"
   },
   "outputs": [],
   "source": [
    "X_tokenized = []\n",
    "\n",
    "for clause in X:\n",
    "  X_tokenized.append(tokenizer(clause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tokens = 0\n",
    "\n",
    "for clause in X_tokenized:\n",
    "    for token in clause:\n",
    "        number_of_tokens += 1\n",
    "\n",
    "print(number_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZe37tKIVpGw"
   },
   "source": [
    "**With Keras text_to_word_sequence() method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-yvTIAGRwq3"
   },
   "outputs": [],
   "source": [
    "X_tokenized = []\n",
    "\n",
    "for clause in X:\n",
    "  X_tokenized.append(text_to_word_sequence(clause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tokens = 0\n",
    "\n",
    "for clause in X_tokenized:\n",
    "    for token in clause:\n",
    "        number_of_tokens += 1\n",
    "\n",
    "print(number_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEa-zqpRVw7f"
   },
   "source": [
    "**With Keras Tokenizer() class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting number of distinct words\n",
    "unique_words = []\n",
    "\n",
    "for clause in X_tokenized:\n",
    "  for token in clause:\n",
    "    if token not in unique_words:\n",
    "      unique_words.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PagppqBOacI1"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(unique_words)+1, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([value, key] for (key, value) in tokenizer.word_index.items())\n",
    "\n",
    "def decode(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "\n",
    "decode(X_tokenized[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting number of all words\n",
    "count = Counter()\n",
    "\n",
    "for clause in X_tokenized:\n",
    "    for word in clause:\n",
    "        count[word] += 1\n",
    "count\n",
    "\n",
    "# for key, value in count.items():\n",
    "#     if value >= 50:\n",
    "#         print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fasttext pretrained embeddings from laptop \n",
    "!wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "target = 'v0.9.2.zip'\n",
    "\n",
    "handle = zipfile.ZipFile(target)\n",
    "handle.extractall()\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../fastText-0.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save_model('cc.fa.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "\n",
    "def pkl_embeddings(word_embedding, unique_words, dimension):\n",
    "    ft = fasttext.load_model('E:/fastText/cc.fa.300.bin')\n",
    "    \n",
    "    if dimension == 300:\n",
    "        print('trying to make embeddings of dimension == 300')\n",
    "        for word in unique_words:\n",
    "          word_embedding[dimension][word] = ft.get_word_vector(word)\n",
    "        \n",
    "        with open(f'C:/Users/Pishtaz/Desktop/Thesis_Code/fasttext_embedding_{dimension}_pretrained.pickle','wb') as f:\n",
    "                pickle.dump(word_embedding, f) \n",
    "        \n",
    "    elif dimension == 200:\n",
    "        print('trying to make embeddings of dimension == 300')\n",
    "\n",
    "        fasttext.util.reduce_model(ft, 200)\n",
    "        \n",
    "        for word in unique_words:\n",
    "            word_embedding[dimension][word] = ft.get_word_vector(word)\n",
    "            \n",
    "        with open(f'C:/Users/Pishtaz/Desktop/Thesis_Code/fasttext_embedding_{dimension}_pretrained.pickle','wb') as f:\n",
    "            pickle.dump(word_embedding, f)\n",
    "        \n",
    "    elif dimension == 100:\n",
    "        print('trying to make embeddings of dimension == 300')\n",
    "\n",
    "        fasttext.util.reduce_model(ft, 100)\n",
    "            \n",
    "        for word in unique_words:\n",
    "            word_embedding[dimension][word] = ft.get_word_vector(word)\n",
    "\n",
    "        with open(f'C:/Users/Pishtaz/Desktop/Thesis_Code/fasttext_embedding_{dimension}_pretrained.pickle','wb') as f:\n",
    "            pickle.dump(word_embedding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "ft = fasttext.load_model('E:/fastText/cc.fa.300.bin')\n",
    "print('loaded persian fastText')\n",
    "\n",
    "word_embedding = {300:{}, 200:{}, 100:{}}\n",
    "\n",
    "fasttext.util.reduce_model(ft, 100)\n",
    "print('reduced embedding dimension')\n",
    "\n",
    "for word in unique_words:\n",
    "  word_embedding[100][word] = ft.get_word_vector(word)\n",
    "\n",
    "print('loaded word embeddings')\n",
    "\n",
    "with open(f'C:/Users/Pishtaz/Desktop/Thesis_Code/fasttext_embedding_{100}_pretrained.pickle','wb') as f:\n",
    "        pickle.dump(word_embedding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = {300:{}, 200:{}, 100:{}}\n",
    "\n",
    "pkl_embeddings(word_embedding, unique_words, 300)\n",
    "pkl_embeddings(word_embedding, unique_words, 200)\n",
    "pkl_embeddings(word_embedding, unique_words, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(directory, dimension):\n",
    "    word_embedding = {300:{}, 200:{}, 100:{}}\n",
    "    \n",
    "    with open(os.path.join(directory,f'fasttext_embedding_{dimension}_pretrained.pickle'),'rb') as f:\n",
    "        word_embedding[dimension] = pickle.load(f)\n",
    "        \n",
    "    return word_embedding[dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/Pishtaz/Desktop/Thesis_Code/embedding'\n",
    "emb_dimension = 300\n",
    "\n",
    "word_embedding = load_embedding(directory, emb_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training fastText on our Corpus:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_unsupervised = fasttext.train_unsupervised('/content/drive/MyDrive/Colab Notebooks/6. Thesis/fastText-UnsupervisedTraining.txt',\n",
    "                                              'cbow', minn=3, maxn=6, dim=300, epoch=5, lr=0.001, thread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ft_unsupervised.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"persian_stopwords.json\",'r',encoding = \"utf-8-sig\") as f:\n",
    "    persian_sw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(X_tokenized, stop_words):\n",
    "\n",
    "    for sw in stop_words:\n",
    "        for sentence in X_tokenized:\n",
    "            try:\n",
    "                while True:\n",
    "                    sentence.remove(sw)\n",
    "            except ValueError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stop_words(X_tokenized, persian_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = tokenizer.texts_to_sequences(X_tokenized)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "x_hSRRfUh9_d",
    "outputId": "1f4b9771-7dee-4f20-84b1-c71a0d8c493f"
   },
   "outputs": [],
   "source": [
    "totalNumWords = [len(line) for line in X_tokenized]\n",
    "plt.hist(totalNumWords,bins = np.arange(0,120,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3yM1qqPiKEx",
    "outputId": "c8d40076-f81b-4acf-f0d5-41784c65bd7e"
   },
   "outputs": [],
   "source": [
    "max(totalNumWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post', truncating='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, emb_dimension))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, emb_dimension))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    #word is the key and i is the value of tokenizer.word_index.items() dictionary\n",
    "    embedding_vector = word_embedding[emb_dimension].get(word)\n",
    "    \n",
    "    if index < vocab_size:\n",
    "        if embedding_vector is not None:\n",
    "                #words not found in embedding index will be all-zeros\n",
    "                embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y,\n",
    "                                                    test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='minority', k_neighbors=7)\n",
    "X_sm, y_sm = smote.fit_resample(X_train, y_train)\n",
    "count = Counter(y_sm)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and testing various neural networks :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option1: LSTM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, emb_dimension, input_length = max_length,\n",
    "                    weights = [embedding_matrix], trainable = False))\n",
    "\n",
    "model.add(LSTM(64, dropout = 0.5, recurrent_dropout = 0.25, return_sequences = True))\n",
    "model.add(LSTM(32, dropout = 0.5, recurrent_dropout = 0.25, return_sequences = True))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(X_train, y_train, epochs = 30,\n",
    "                batch_size = 32, validation_split=0.2)\n",
    "finish = time.time()\n",
    "\n",
    "print(finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LSTM'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.reshape(y_pred,(y_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LSTM'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_model = pd.DataFrame(columns=['accuracy', 'val_accuracy', 'loss', 'val_loss'])\n",
    "\n",
    "for col in df_log_model.columns:\n",
    "    df_log_model[col] = hist.history[col]\n",
    "    \n",
    "df_log_model.to_csv(base_dir+f'{model_name}-reports-model.csv', index=0)\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res-round.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option2: BiLSTM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, emb_dimension, input_length = max_length,\n",
    "                    weights = [embedding_matrix], trainable = False))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True,\n",
    "                             name='bilstm_1',dropout=0.5,recurrent_dropout=0.1)))\n",
    "model.add(LSTM(16, dropout = 0.5, recurrent_dropout = 0.25, name='lstm_1', return_sequences = True))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu', name = \"dense_1\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation = 'relu', name = \"dense_2\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', name = \"output\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs = 30,\n",
    "                batch_size = 32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BiLSTM'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.reshape(y_pred,(y_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BiLSTM'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_model = pd.DataFrame(columns=['accuracy', 'val_accuracy', 'loss', 'val_loss'])\n",
    "\n",
    "for col in df_log_model.columns:\n",
    "    df_log_model[col] = hist.history[col]\n",
    "    \n",
    "df_log_model.to_csv(base_dir+f'{model_name}-reports-model.csv', index=0)\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res-round.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option3: FFNN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, emb_dimension, input_length = max_length,\n",
    "                    weights = [embedding_matrix], trainable = False))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs = 30,\n",
    "                batch_size = 32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'FFNN'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.reshape(y_pred,(y_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'FFNN'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_model = pd.DataFrame(columns=['accuracy', 'val_accuracy', 'loss', 'val_loss'])\n",
    "\n",
    "for col in df_log_model.columns:\n",
    "    df_log_model[col] = hist.history[col]\n",
    "    \n",
    "df_log_model.to_csv(base_dir+f'{model_name}-reports-model.csv', index=0)\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res-round.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option4: GRU:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, emb_dimension, input_length = max_length,\n",
    "                    weights = [embedding_matrix], trainable = False))\n",
    "\n",
    "model.add(GRU(64, dropout = 0.5, recurrent_dropout = 0.25, return_sequences = True, name=\"gru_1\"))\n",
    "model.add(GRU(32, dropout = 0.5, recurrent_dropout = 0.25, return_sequences = True, name=\"gru_2\"))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu', name=\"dense_1\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16, activation = 'relu', name=\"dense_2\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(X_train, y_train, epochs = 30,\n",
    "                batch_size = 32, validation_split=0.2)\n",
    "finish = time.time()\n",
    "\n",
    "print(finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'GRU'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.reshape(y_pred,(y_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'GRU'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_model = pd.DataFrame(columns=['accuracy', 'val_accuracy', 'loss', 'val_loss'])\n",
    "\n",
    "for col in df_log_model.columns:\n",
    "    df_log_model[col] = hist.history[col]\n",
    "    \n",
    "df_log_model.to_csv(base_dir+f'{model_name}-reports-model.csv', index=0)\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res-round.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option5: CNN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import  Conv1D, MaxPooling1D, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, emb_dimension, input_length = max_length,\n",
    "                    weights = [embedding_matrix], trainable = False))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=4, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=8, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs = 30,\n",
    "                batch_size = 32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CNN'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.reshape(y_pred,(y_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CNN'\n",
    "base_dir = 'D:/Construction Management/Thesis/Extracted Paper/Figures/'\n",
    "\n",
    "df_log_model = pd.DataFrame(columns=['accuracy', 'val_accuracy', 'loss', 'val_loss'])\n",
    "\n",
    "for col in df_log_model.columns:\n",
    "    df_log_model[col] = hist.history[col]\n",
    "    \n",
    "df_log_model.to_csv(base_dir+f'{model_name}-reports-model.csv', index=0)\n",
    "\n",
    "df_log_res = pd.DataFrame(columns=['y_test', 'y_pred'])    \n",
    "df_log_res['y_test'] = list(y_test)\n",
    "df_log_res['y_pred'] = list(np.ravel(y_pred))\n",
    "\n",
    "df_log_res.to_csv(base_dir+f'{model_name}-reports-res-round.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _emb_matrix_wv(index_dict, word_vectors, **params):\n",
    "\n",
    "    print(\"creating embedding matrix for word2vec\")\n",
    "    \n",
    "    vocab_size = len(index_dict) + 1\n",
    "\n",
    "    embedding_weights = np.zeros((vocab_size, 300))\n",
    "\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "        \n",
    "    return vocab_size, embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _emb_dict_wv(wv_model, **params):\n",
    "\n",
    "    if wv_model is not None:\n",
    "\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(wv_model.wv.key_to_index.keys(), allow_update=True)\n",
    "\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}\n",
    "        w2vec = {word: wv_model.wv.get_vector(word) for word in w2indx.keys()}\n",
    "        \n",
    "        return w2indx, w2vec\n",
    "\n",
    "    else:\n",
    "        print('Word2Vec model is not trained yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(X_tokenized, **params):\n",
    "    print(\"training word2vec model\")\n",
    "\n",
    "    wv_model = gensim.models.word2vec.Word2Vec(sentences = X_tokenized,\n",
    "                                            vector_size = 300,\n",
    "                                            window = 5,\n",
    "                                            min_count = 3,\n",
    "                                            sg = 1,\n",
    "                                            epochs = 5,)\n",
    "\n",
    "    wv_model.build_vocab(X_tokenized)\n",
    "\n",
    "    wv_model.train(X_tokenized, total_examples=wv_model.corpus_count, epochs = 5,)\n",
    "    wv_model.save(\"C:/Users/Pishtaz/Desktop/other/Thesis/Risky-Clauses-NLP/embedding-jupyter/w2v_model.pkl\")\n",
    "\n",
    "    index_dict, word_vectors = _emb_dict_wv(X = X_tokenized, wv_model = wv_model)\n",
    "\n",
    "    return index_dict, word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_network_info(index_dict,\n",
    "                        vocab_size,\n",
    "                        embedding_weights):\n",
    "\n",
    "\n",
    "    base_dir = \"C:/Users/Pishtaz/Desktop/other/Thesis/Risky-Clauses-NLP/embedding-jupyter/\"\n",
    "    with open(base_dir + 'index_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(index_dict, handle)\n",
    "\n",
    "    with open(base_dir + 'vocab_size.pkl', 'wb') as handle:\n",
    "        pickle.dump(vocab_size, handle)\n",
    "\n",
    "    with open(base_dir + 'embedding_weights.pkl', 'wb') as handle:\n",
    "        pickle.dump(embedding_weights, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_embedding(X_tokenized,**params):\n",
    "\n",
    "    index_dict, word_vectors = train_word2vec(X_tokenized, **params)\n",
    "    vocab_size, embedding_weights = _emb_matrix_wv(index_dict,\n",
    "                                                word_vectors,\n",
    "                                                **params)\n",
    "\n",
    "    _save_network_info(index_dict, vocab_size, embedding_weights)\n",
    "    \n",
    "    return index_dict, vocab_size, embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict, vocab_size, embedding_weights = create_word2vec_embedding(X_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"persian_stopwords.json\",'r',encoding = \"utf-8-sig\") as f:\n",
    "    persian_sw = json.load(f)\n",
    "    \n",
    "def remove_stop_words(X_tokenized, stop_words):\n",
    "\n",
    "    for sw in stop_words:\n",
    "        for sentence in X_tokenized:\n",
    "            try:\n",
    "                while True:\n",
    "                    sentence.remove(sw)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "remove_stop_words(X_tokenized, persian_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(X_tokenized, w2indx):\n",
    "\n",
    "    parsed = []\n",
    "    for sentence in X_tokenized:\n",
    "        indexed_sentence = []\n",
    "\n",
    "        for token in sentence:\n",
    "            try:\n",
    "                indexed_sentence.append(w2indx[token])\n",
    "            except:\n",
    "                indexed_sentence.append(0)\n",
    "\n",
    "        parsed.append(indexed_sentence)\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(parsed, **params):\n",
    "\n",
    "    print(\"padding the parsed sentences...\")\n",
    "    \n",
    "    X = pad_sequences(parsed, maxlen = 50,\n",
    "                              padding = 'post',\n",
    "                              truncating = 'post')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(X_train, Y_train, **params):\n",
    "    k_nbrs_overs = params.get(\"k_nbrs_overs\")\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy = 'minority', k_neighbors = 5)\n",
    "\n",
    "    X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_(X, Y, **params):\n",
    "    print(\"splitting data to train and test...\")\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,\n",
    "         shuffle = True, stratify = Y)\n",
    "\n",
    "    Y_train, Y_test = np.array(Y_train), np.array(Y_test)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pad_x_train_test(X_tokenized, Y, index_dict, vocab_size, embedding_weights, **params):\n",
    "\n",
    "    print (\"Trying to parse_pad_and_save_x_train_test...\")\n",
    "\n",
    "    X_tokenized = parsing(X_tokenized, index_dict, **params)\n",
    "    X = padding(X_tokenized, **params)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split_(X, Y, **params)\n",
    "#     X_train, Y_train = oversample(X_train, Y_train, **params)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = parse_pad_x_train_test(X_tokenized, y, index_dict, vocab_size, embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, emb_dimension, input_length = max_length,\n",
    "                    weights = [embedding_weights], trainable = False))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True,\n",
    "                             name='bilstm_1',dropout=0.5,recurrent_dropout=0.1)))\n",
    "model.add(LSTM(16, dropout = 0.5, recurrent_dropout = 0.25, name='lstm_1', return_sequences = True))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu', name = \"dense_1\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation = 'relu', name = \"dense_2\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', name = \"output\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, Y_train, epochs = 30,\n",
    "                batch_size = 32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Model\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,Y_test,batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.round(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.reshape(Y_pred,(Y_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,Y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Thesis_Code_BiLSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
